{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09588208-7596-48c4-a6b5-33ab5844aff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import numba.cuda\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6265126-a2dd-4b8b-8adb-1fbb3173dcd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if this doesn't work you don't have a compatible GPU!\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0700e-787f-4da9-b5b4-230c138bf05d",
   "metadata": {},
   "source": [
    "# Stochastic Differential Equations (SDEs) and Euler-Maruyama Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a91b7-4fed-4a87-b888-2279cb3dfe63",
   "metadata": {},
   "source": [
    "## What is a Stochastic Differential Equation (SDE)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6556c-8df9-48fe-9853-f14f861cb56d",
   "metadata": {},
   "source": [
    "A Stochastic Differential Equation (SDE) is a differential equation in which one or more of the terms is a stochastic process, resulting in a solution that is itself a stochastic process. SDEs are used to model systems that are influenced by random noise or uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a344f299-e28a-470d-a8f0-7bb8ba01342d",
   "metadata": {},
   "source": [
    "## Brownian Motion with Drift and Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382b591-b873-4ed9-97ba-a5ca224fc636",
   "metadata": {},
   "source": [
    "Brownian motion with drift and volatility models the random movement of a particle or financial asset prices over time, incorporating both a systematic trend and random variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f00b21-7513-4c06-82a6-b3282ab8a414",
   "metadata": {},
   "source": [
    "## Why are SDEs Useful in Finance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51efbfa-0eed-402a-810e-efd0029d804b",
   "metadata": {},
   "source": [
    "SDEs model the price movements of financial assets, aid in risk management, and contribute to strategies in portfolio optimization, interest rate modeling, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51837ac-0b28-4b5f-ad2d-d60a649177dc",
   "metadata": {},
   "source": [
    "## Euler Method for Simulating SDEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75067d3-ec4e-4fe2-9591-7b296f4ea8ff",
   "metadata": {},
   "source": [
    "The Euler-Maruyama method is a numerical technique used to approximate the solutions of SDEs by discretizing time into small steps and iteratively calculating the value of the stochastic process at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bae85b-8621-4a6d-bb93-e2f5dc739de4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stochastic Differential Equations (SDEs) with Numba and CuPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f3fa2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed028969-2348-4c9d-a700-8432c907491d",
   "metadata": {},
   "source": [
    "This notebook demonstrates the simulation of Stochastic Differential Equations (SDEs) using Python. It includes explanations and code for using traditional methods, optimizations with Numba, and leveraging GPU acceleration with CuPy and Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414e638-5c2d-4fdb-89dc-5a4412d50e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "mu = 0.1  # Drift coefficient\n",
    "sigma = 0.2  # Volatility coefficient\n",
    "X0 = 1.0  # Initial value of the process\n",
    "T = 1.0  # Total time\n",
    "N = 1000  # Number of time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9034c6-3b0d-4694-895b-8baa8657499c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt = T/N  # Time step size\n",
    "\n",
    "# Time vector\n",
    "timesteps = np.linspace(0, T, N)\n",
    "\n",
    "# Pre-generated matrix of standard normal variates\n",
    "np.random.seed(42)  # Seed for reproducibility\n",
    "dW = np.random.normal(0, np.sqrt(dt), size=N)\n",
    "\n",
    "# Initialize the process values array\n",
    "X = np.zeros(N)\n",
    "X[0] = X0\n",
    "\n",
    "# Naive un-optimized implementation of Euler-Maruyama method on CPU\n",
    "for i in range(1, N):\n",
    "    X[i] = X[i-1] + mu*X[i-1]*dt + sigma*X[i-1]*dW[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac023ca2-ac65-4ff6-a743-8b351cdab97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using plotly to visualise the data via Pandas dataframe\n",
    "X_plot_data=pd.DataFrame({'Time (yrs)':timesteps,'Price':X})\n",
    "px.line(X_plot_data,x='Time (yrs)',y='Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e262b-ee08-44e8-a52d-b203e91080fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "That code illustrates how the simulation works, but it's not very performant. It's also happening on the CPU, which means it can't be included in an ML workflow that's happening on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db958e9a-8c58-47a9-8e9e-92656a8a7428",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simulation using CuPy and Numba on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f5d67-1a64-488b-9a35-8c670b5d0c73",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CUDA Threads and Blocks\n",
    "\n",
    "**CUDA** (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA for general computing on its own GPUs (Graphics Processing Units). Understanding the concepts of threads and blocks is crucial for leveraging CUDA's capabilities for parallel computations.\n",
    "\n",
    "### CUDA Threads:\n",
    "In CUDA, a **thread** is the smallest unit of execution. Each thread has its own set of registers and local memory, and it executes a specified function, known as a kernel. Threads are extremely lightweight and are designed to execute in parallel, allowing for the efficient parallel processing of tasks. The programmer can define a multidimensional grid of threads, where each thread is uniquely identified by its index within the grid.\n",
    "\n",
    "Threads are grouped into blocks, which brings us to the next concept.\n",
    "\n",
    "### CUDA Blocks:\n",
    "A **block** is a group of threads that execute the same kernel code. Threads within the same block can communicate with each other and synchronize their execution, facilitating efficient parallel algorithms. This is primarily done through shared memory available to all threads within a block, which offers much faster access times compared to global memory accessible by all threads and blocks.\n",
    "\n",
    "Blocks are organized into a grid, where each block in the grid can be identified by its block index. Blocks can also be multidimensional, allowing for a flexible mapping of computation tasks to the CUDA architecture.\n",
    "\n",
    "### Key Points:\n",
    "- **Parallel Execution**: Threads within the same block can execute in parallel, and multiple blocks can execute simultaneously on different SMs (Streaming Multiprocessors) within a GPU, harnessing the full power of the GPU for parallel computations.\n",
    "- **Memory Hierarchy**: CUDA provides a hierarchy of memory accessible by threads, including local memory (per thread), shared memory (per block), and global memory (across all threads and blocks). Efficient use of this memory hierarchy is key to optimizing CUDA applications.\n",
    "- **Synchronization**: Threads within a block can be synchronized, ensuring that all threads reach certain points in the execution together. This is crucial for operations that depend on the results of other threads within the same block.\n",
    "\n",
    "Understanding and effectively utilizing threads and blocks is fundamental to achieving high performance in CUDA applications. By dividing tasks into manageable chunks that can be processed in parallel by threads organized into blocks, developers can achieve significant speedups for a wide range of computational tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14436a15-e28e-4779-b132-4b6d44e976fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SDE simluation on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5e0a3-134e-4ea4-a6c4-1e34ad94c60a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here are the parameters we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a11b8e-4619-43ea-8c27-94569c79aeb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = 1000000  # lets pick a very large number of paths, something dr evil would be proud of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6a299-56b7-46d8-b883-33e4188e92a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note that this is a lot of data! 1024 time steps is fairly high resolution for an SDE simulation, and we are simulating 1m paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be251c4-91f1-4076-8c5d-ae88d660f59f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_size_mbs=N*paths*8/1024/1024\n",
    "print(f'the size in memory of the generated MC paths will be {data_size_mbs:.2f} Mbs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa7c4e-204f-4299-944e-d0c853c3cfad",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here is the GPU Kernel. Note the decorator `@numba.cuda.jit`, which tells the Python interpreter this is not a normal function, but rather something that is intended to be treated as a CUDA kernel. the `import numba` statement at the top of this workbook is what makes this decorator available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a44ad-1149-4b95-8014-5a8686dc109c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@numba.cuda.jit\n",
    "def sde_simulation_cuda(result, random_numbers, mu, sigma, X0, T, N):\n",
    "    idx = numba.cuda.grid(1) # this tells us which GPU thread we're on-- corresponding to the \n",
    "    dt = T / N\n",
    "    if idx < result.shape[0]: # this if statement is important since \n",
    "        X = X0\n",
    "        for i in range(1, N):\n",
    "            dW = random_numbers[i-1,idx]  # Use pre-generated random number\n",
    "            X += mu * X * dt + sigma * X * dW\n",
    "            result[i,idx] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf4542-6a73-486e-83f6-c1386bde3c9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "As with the cpu code above, we are simulating the SDE by discretizing in the time domain. Note however that this code only describes creating a single path-- and we want 1m ! We will actually be calling this function on a large number of CUDA blocks.\n",
    "\n",
    "Two things worth noticing in the code above:\n",
    "1) `idx=cuda.grid(1)` gets the value of the path we're on, an integer ranging from `0` to `paths-1`. cuda.grid is not normally available in a generic Python function, it will only be available here because of the numba decorator.\n",
    "2) `if idx<result.shape[0]` is necessary because the number of blocks we launch will be rounded up to ensure we get >= the 1m paths we want. This means that, in the highest block, the last few threads may be more than the 1m datapoints we need. The if statement ensures those specific threads do nothing--otherwise there would be index read/write violations on `result` and `random_numbers`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc5d4d-63c7-4156-8928-4ee43604a774",
   "metadata": {
    "tags": []
   },
   "source": [
    "More on CUDA in a minute.\n",
    "\n",
    "First we need to generate the random variables, which we will do using cupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4a2bf-fcfc-4c35-86cb-3a3c066b505c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_sqrt = cp.sqrt(T / N)\n",
    "random_numbers_gpu = cp.random.normal(0, dt_sqrt, size=(N-1,paths))\n",
    "print(f'shape of data is {random_numbers_gpu.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660898a-a2dc-4826-a19b-d825b7215af9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note how fast that was! We just created 8gb of random data in a few milliseconds! You  can see the memory footprint in the SMI output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b900f5d-14a2-4ff8-8872-c66f1e66448e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a213381-15f9-4cf3-b51c-6907f35acd23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create empty container for results\n",
    "result_gpu = cp.zeros((N,paths), dtype=cp.float32)\n",
    "result_gpu[0,:] = X0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad8766-f6b4-4c39-9429-8f5d59e30120",
   "metadata": {
    "tags": []
   },
   "source": [
    "Running the smi again shows that the memory footprint has increased-- that's the result container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e8cef-a0ac-4e58-9901-b4d228cb6d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccaf539-809e-4884-a650-a3622da18155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create numba objects from the cupy objects. Note that there is no memory copy here!\n",
    "random_numbers_numba = numba.cuda.as_cuda_array(random_numbers_gpu)\n",
    "result_numba = numba.cuda.as_cuda_array(result_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfa054-2a94-44bb-acfb-32d6fcc5bb87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is a somewhat arbitrary choice, and it can be tuned for performance if needed.\n",
    "# 256 is a safe choice to use as default.\n",
    "threads_per_block = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eeeb0c-0d8b-4763-8c31-7d9d70eb93b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this formula determines how many blocks (rounded up) we need for the number of paths.\n",
    "# conceptually, this is `paths / threads_per_block` rounded up to the nearest integer.\n",
    "# this rounding up is important to ensure we have enough blocks for the number of paths.\n",
    "# note that python has // as the rounded-down division operator. \n",
    "blocks_per_grid = (paths + threads_per_block - 1) // threads_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a7981-06d5-4519-8a26-e82c8a94c1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# now we call the kernel. the first time it will do the jit \n",
    "sde_simulation_cuda[blocks_per_grid, threads_per_block](result_numba, random_numbers_numba, mu, sigma, X0, T, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d7dad-b70e-4fa8-987a-84b32ea1fdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# call again, this time there is no jit\n",
    "sde_simulation_cuda[blocks_per_grid, threads_per_block](result_numba, random_numbers_numba, mu, sigma, X0, T, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfdcfa-d85b-42e5-bcda-a8233ccae696",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notice that calling it again takes substantially less time, as we don't have to redo the jit. It's worth noticing that turning the random normals into sde paths is actually way faster than generating the random numbers was. When you're using a highly optimized GPU kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd189eb8-8ffb-4225-be83-85c01199faad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# copying from GPU ram (on the graphics card) to CPU ram (on motherboard)\n",
    "result_host = result_gpu.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776a454-d7b7-4fea-9dee-665866892b21",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is a very slow operation (relatively), since we're copying 8gb of data from the VRAM to regular RAM across the PCIe bus (think: the motherboard slot for GPUs). This slowness illustrates our second reason for wanting to do the MC on GPU. Not only is it way faster than CPU, but if we're using this MC as part of a larger AI workflow, we need the data on the GPU anyway.\n",
    "\n",
    "In reality though it should never be necessary to do copy raw MC scenarios like this, we'll usually only be interested in downstream results (eg trained nn parameters, which are much smaller in size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c20c4-727d-4472-a61b-f49dd78e7ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# using plotly to visualize generated path data as a high resolution \"heatmap\".\n",
    "# you can click and zoom in on parts of the graph to see more detail.\n",
    "# this is 1/10000th of the total data generated.\n",
    "paths_to_plot=100\n",
    "data_for_plotting=pd.DataFrame(result_host,index=np.linspace(0, T, N))\n",
    "data_for_plotting.columns=data_for_plotting.columns.rename('path')\n",
    "data_for_plotting.index=data_for_plotting.index.rename('timestep')\n",
    "data_for_plotting=data_for_plotting.iloc[:,:paths_to_plot]\n",
    "fig=px.line(data_for_plotting)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_traces(line=dict(color='blue'))\n",
    "fig.update_traces(opacity=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8428a3a3-435b-48c7-a899-18e3429b8c2f",
   "metadata": {},
   "source": [
    "This is 1/10000th of the total data generated. And even so, saving this notebook now will make it 13mb because of the amount of data plotly is storing on the JS/frontend side just to fully (ie zoomably) visualize 0.01% of the paths we just generated.\n",
    "\n",
    "I hope you took a moment to zoom in on different parts of the chart above, brownian motion has a serene beauty when you see it in high def."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3481e6-c929-4430-895a-a3826c75d86c",
   "metadata": {},
   "source": [
    "This is just a basic Geometric Brownian motion, however simulating more complex SDEs (including multivariate ones) would work exactly the same way programatically. Follow this pattern and you should have very high performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
